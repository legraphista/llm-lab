{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LoRA's\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique for large language models.  \n",
    "Instead of updating all model parameters during fine-tuning (which can be billions), LoRA adds small trainable \"rank decomposition\" matrices to key layers.  \n",
    "For example, in a 8B parameter model like Llama 3, we might only train 1-2% of parameters using LoRA.\n",
    "\n",
    "\n",
    "Real-world example: If you want to adapt GPT to be better at biology questions, rather than retraining all parameters (expensive!),  \n",
    "LoRA lets you inject small, trainable matrices into attention layers.  \n",
    "These matrices learn to \"steer\" the model's existing knowledge toward biology, while keeping most original parameters frozen.\n",
    "\n",
    "This makes fine-tuning much more efficient in terms of:\n",
    "- Computing requirements (can run on consumer GPUs)\n",
    "- Memory usage (don't need to store full parameter copies)\n",
    "- Training time (fewer parameters to optimize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets torch unsloth ipywidgets scikit-learn numpy plotly pandas hf_transfer\n",
    "import os\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model\n",
    "\n",
    "We will be using the `unsloth/Llama-3.2-3B-Instruct` model.\n",
    "\n",
    "Unsloth is a library that allows us to load models efficiently and train LoRA efficiently.\n",
    "\n",
    "Here are a few models compatible with Unsloth:\n",
    "- `unsloth/Meta-Llama-3.1-8B`      \n",
    "- `unsloth/Meta-Llama-3.1-8B-Instruct`\n",
    "- `unsloth/Meta-Llama-3.1-70B`\n",
    "- `unsloth/Meta-Llama-3.1-405B`    \n",
    "- `unsloth/Mistral-Nemo-Base-2407` \n",
    "- `unsloth/Mistral-Nemo-Instruct-2407`\n",
    "- `unsloth/mistral-7b-v0.3`       \n",
    "- `unsloth/mistral-7b-instruct-v0.3`\n",
    "- `unsloth/Phi-3.5-mini-instruct`  \n",
    "- `unsloth/Phi-3-medium-4k-instruct`\n",
    "- `unsloth/gemma-2-9b`\n",
    "- `unsloth/gemma-2-27b`       \n",
    "\n",
    "You can find more models at https://huggingface.co/unsloth/models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.546 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# max context length\n",
    "max_seq_length = 2048\n",
    "\n",
    "dtype = None # auto\n",
    "\n",
    "# load in 4bit or 8bit if the model doesn't fit on your GPU\n",
    "load_in_4bit = False\n",
    "load_in_8bit = False\n",
    "\n",
    "# set a random seed for reproducibility\n",
    "random_state = 42\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    load_in_8bit = load_in_8bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset\n",
    "\n",
    "We will be fine-tuning on the [RoBiology/RoBiologyDataChoiceQA](https://huggingface.co/datasets/RoBiology/RoBiologyDataChoiceQA) dataset.\n",
    "\n",
    "This fine-tune will only use single-choice questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_number', 'question', 'type', 'options', 'grade', 'stage', 'year', 'right_answer', 'source', 'id_in_source', 'dupe_id'],\n",
       "        num_rows: 11368\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_number', 'question', 'type', 'options', 'grade', 'stage', 'year', 'right_answer', 'source', 'id_in_source', 'dupe_id'],\n",
       "        num_rows: 1376\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_number', 'question', 'type', 'options', 'grade', 'stage', 'year', 'right_answer', 'source', 'id_in_source', 'dupe_id'],\n",
       "        num_rows: 1388\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"RoBiology/RoBiologyDataChoiceQA\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for single-choice questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question_number', 'question', 'type', 'options', 'grade', 'stage', 'year', 'right_answer', 'source', 'id_in_source', 'dupe_id'],\n",
       "    num_rows: 4860\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = dataset['train'].filter(lambda x: x['type'] == 'single-choice')\n",
    "validation_ds = dataset['validation'].filter(lambda x: x['type'] == 'single-choice')\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate chat prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_prompt(instruction: str, input: str, response: str|None = None):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": input},\n",
    "            *([{\"role\": \"assistant\", \"content\": response}] if response else []),\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Dec 2024\n",
      "\n",
      "Answer the question based on the given options. Respond by writing only the letter of the correct answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Celula procariotÄƒ:\n",
      "A. caracterizea zÄƒ virusurile, bacteriile È™i algele alba stre verzi\n",
      "B. conÈ›ine pept idioglican Ã®n compoziÈ›ia membranei celulare\n",
      "C. nu prezintÄƒ perete celular\n",
      "D. materialul nuclear este o moleculÄƒ circularÄƒ de ADN bicatenar<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "D<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "instuction = \"Answer the question based on the given options. Respond by writing only the letter of the correct answer.\"\n",
    "\n",
    "print(create_chat_prompt(\n",
    "    instuction,\n",
    "    \"\\n\".join([train_ds[0][\"question\"]] + train_ds[0][\"options\"]),\n",
    "    train_ds[0][\"right_answer\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_ds.map(lambda x: {\n",
    "    \"chat\": create_chat_prompt(instuction, '\\n'.join([x['question']] + x['options']), x['right_answer']) \n",
    "})\n",
    "\n",
    "validation = validation_ds.map(lambda x: {\n",
    "    \"chat\": create_chat_prompt(instuction, '\\n'.join([x['question']] + x['options']), x['right_answer']) \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to google drive (if running in Colab) to persist model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_MOUNTED = True\n",
    "except Exception:\n",
    "    DRIVE_MOUNTED = False\n",
    "\n",
    "if not IN_COLAB:\n",
    "    final_model_path = './lora_model'\n",
    "    checkpoints_path = './checkpoints'\n",
    "else:\n",
    "    if DRIVE_MOUNTED:\n",
    "        final_model_path = '/content/drive/MyDrive/Colab Notebooks/finetuning/lora_model'\n",
    "        checkpoints_path = '/content/drive/MyDrive/Colab Notebooks/finetuning/checkpoints'\n",
    "    else:\n",
    "        print(\"!\" * 100)\n",
    "        print(\"DRIVE NOT MOUNTED, all saved files will be deleted after the session ends\")\n",
    "        print(\"!\" * 100)\n",
    "        final_model_path = './lora_model'\n",
    "        checkpoints_path = './checkpoints'\n",
    "\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "os.makedirs(checkpoints_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting lora parameters:\n",
    "\n",
    "#### r:   \n",
    "The rank of the LoRA update matrices. This controls the size of the low-rank matrices used to update the model weights.  \n",
    "Higher values (e.g. 32, 64) mean more trainable parameters and potentially better model quality, but require more memory and training time.  \n",
    "Lower values (e.g. 8, 16) are more efficient but may limit model improvements.  \n",
    "\n",
    "#### target_modules\n",
    "Specifies which layers of the model to apply LoRA updates to. Here we target:  \n",
    "    - q_proj/k_proj/v_proj/o_proj: The attention mechanism layers that process queries, keys, values and outputs  \n",
    "    - gate_proj/up_proj/down_proj: The feedforward neural network layers that process each token's representations  \n",
    "Targeting more layers allows for more comprehensive fine-tuning but increases memory usage and training time.  \n",
    "\n",
    "#### lora_alpha\n",
    "The scaling factor applied to LoRA updates before they're added to the original weights.  \n",
    "Usually set equal to r to maintain a consistent scale of updates.  \n",
    "A larger alpha relative to r means the LoRA updates have more influence on the final model behavior.  \n",
    "\n",
    "#### lora_dropout\n",
    "Probability of randomly dropping LoRA updates during training. Higher values (e.g. 0.1) can help prevent overfitting but slow down training. \n",
    "Set to 0 here for maximum training speed.  \n",
    "  \n",
    "#### bias\n",
    "Controls whether to train the bias terms in the model. Options are:  \n",
    "- \"none\": Don't train any bias terms (fastest)  \n",
    "- \"all\": Train all bias terms  \n",
    "- \"lora_only\": Only train bias terms in LoRA layers  \n",
    "\n",
    "#### use_gradient_checkpointing\n",
    "Memory optimization that trades computation for memory by recomputing activations during the backward pass instead of storing them.  \n",
    "The \"unsloth\" setting provides enhanced memory savings of about 30% VRAM usage compared to standard checkpointing.  \n",
    "\n",
    "#### random_state\n",
    "Seed value for random number generation to ensure reproducible results across training runs.  \n",
    "Affects initialization of LoRA weights and any random operations during training.  \n",
    "\n",
    "#### use_rslora\n",
    "Whether to use Rank-Stabilized LoRA, which adds techniques to improve training stability for larger rank values.  \n",
    "Can help prevent training instability but adds some computational overhead.  \n",
    "\n",
    "#### loftq_config\n",
    "Configuration settings for LoftQ (Low-rank Factorization with Quantization), which combines low-rank approximation with quantization to reduce model size. \n",
    "Set to None here to disable LoftQ.  \n",
    "\n",
    "\n",
    "More details can be found here: https://docs.unsloth.ai/basics/lora-parameters-encyclopedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# setting up LoRA parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = random_state,\n",
    "    use_rslora = False,  # support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize trainer:\n",
    "\n",
    "There are several key training strategies for language models:\n",
    " \n",
    "|  | SFT | RLHF | DPO | ORPO |\n",
    "|--------|-----|------|-----|------|\n",
    "| Data Format | Input-output pairs:<br>{prompt: str, response: str} | SFT data + preferences:<br>{prompt: str, response1: str, response2: str, preferred: int} | Preferred/rejected pairs:<br>{prompt: str, chosen: str, rejected: str} | One accepted + multiple rejected:<br>{prompt: str, accepted: str, rejected: List[str]} |\n",
    "| Pros | â€¢ Simple and straightforward<br>â€¢ Requires less data<br>â€¢ Fast training process<br>â€¢ Direct control over outputs<br>â€¢ Less catastrophic forgetting | â€¢ Optimizes for preferences<br>â€¢ Learns complex behaviors<br>â€¢ Strong value alignment<br>â€¢ Reduces unwanted outputs<br>â€¢ Better retention of base model knowledge | â€¢ Simpler than RLHF<br>â€¢ More stable training<br>â€¢ No reward model needed<br>â€¢ Efficient preference learning<br>â€¢ Moderate forgetting risk | â€¢ Learns from multiple negatives<br>â€¢ Better at avoiding bad behaviors<br>â€¢ Efficient use of rejection data<br>â€¢ Stable like DPO<br>â€¢ Good knowledge preservation |\n",
    "| Cons | â€¢ May learn data flaws<br>â€¢ Limited to explicit examples<br>â€¢ Can't learn from feedback<br>â€¢ Used in this notebook<br>â€¢ May still forget some base capabilities | â€¢ Complex pipeline<br>â€¢ Needs lots of preference data<br>â€¢ Computationally expensive<br>â€¢ Training can be unstable<br>â€¢ Risk of forgetting during reward learning | â€¢ Still needs preference pairs<br>â€¢ Can't handle multiple rejection levels<br>â€¢ Less flexible than RLHF<br>â€¢ Needs careful data curation<br>â€¢ May forget original task abilities | â€¢ Complex dataset structure<br>â€¢ Needs multiple rejections per prompt<br>â€¢ May overemphasize negatives<br>â€¢ Newer, less tested method<br>â€¢ Forgetting patterns not well studied |\n",
    "\n",
    "We will use Supervised Fine-Tuning (SFT) in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437335f4bbdf4e26997e0f493f0c7d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/4860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396d872b2e89451dabcee1497bb231bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "\n",
    "    train_dataset = train,\n",
    "    eval_dataset=validation,\n",
    "    dataset_text_field = \"chat\",\n",
    "    \n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        # save every 100 steps \n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "\n",
    "        # validation steps\n",
    "        do_eval = True,\n",
    "        eval_on_start = True,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 100,\n",
    "        \n",
    "        # batching\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "\n",
    "        # how much to train\n",
    "        num_train_epochs = 5, # 5 epochs\n",
    "        # max_steps = 60, # or 60 steps\n",
    "\n",
    "        # learning rate\n",
    "        learning_rate = 2e-4,\n",
    "\n",
    "        # fp16 or bfloat16\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "\n",
    "        # logging\n",
    "        logging_steps = 1,\n",
    "\n",
    "        # optimizer\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = random_state,\n",
    "\n",
    "        # output\n",
    "        output_dir = checkpoints_path,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training\n",
    "\n",
    "Grab some popcorn, this will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 4,860 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 3,035\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3035' max='3035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3035/3035 28:35, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.655867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.984900</td>\n",
       "      <td>1.265938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.120500</td>\n",
       "      <td>1.203853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>1.161356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.927900</td>\n",
       "      <td>1.130876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>1.107503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.058500</td>\n",
       "      <td>1.087961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>1.084503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>1.065076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>1.066231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.866100</td>\n",
       "      <td>1.049157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>1.041390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>1.030758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>1.064802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>1.069437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>1.059178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.729500</td>\n",
       "      <td>1.065066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>1.052210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.719400</td>\n",
       "      <td>1.046914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>1.118199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>1.110782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>1.130981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.589800</td>\n",
       "      <td>1.118024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>1.128789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>1.119020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>1.211534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>1.215310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>1.223829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>1.223285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>1.227496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>1.224185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model as a LoRA of the original model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_model/tokenizer_config.json',\n",
       " './lora_model/special_tokens_map.json',\n",
       " './lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(final_model_path) # Local saving\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save it for the VLLM framework as:\n",
    "- a merged model (16bit or 4bit)\n",
    "- just the LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save it for the llama.cpp framework / Ollama / GGUF format as:\n",
    "- 8bit Q8_0\n",
    "- 16bit f16\n",
    "- 4-bit q4_k_m\n",
    "\n",
    "See full support for quantization here: https://github.com/unslothai/unsloth/wiki#gguf-quantization-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
