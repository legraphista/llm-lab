{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LLMs locally\n",
    "\n",
    "We will be using the `transformers` library to run LLMs locally with PyTorch as backend.  \n",
    "This notebook will require a GPU with at least 12GB of VRAM.  \n",
    "It is recommended to use Google Colab with at least a T4 GPU runtime.\n",
    "\n",
    "\n",
    "This notebook will cover the following topics:\n",
    "- Running LLMs locally\n",
    "- VRAM limitations\n",
    "- Quantization\n",
    "- Using multimodal models\n",
    "- Structured outputs with JSONformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers hf_transfer torch accelerate bitsandbytes pillow\n",
    "\n",
    "# - transformers - high-level API for working with models\n",
    "# - hf_transfer - allows for faster downloads from Hugging Face\n",
    "# - torch - Backend library for transformers\n",
    "# - accelerate - library for distributed inference / training - allows us to use device=\"auto\"\n",
    "# - bitsandbytes - allows for quantization of models on the fly\n",
    "# - pillow - allows for image processing\n",
    "\n",
    "# enable hf_transfer for faster downloads\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype='auto', device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype='auto', device_map=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a message to send to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a message to send to the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"When was Alan Turing born?\"}\n",
    "]\n",
    "\n",
    "# generate the chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "\n",
    "# tokenize the text in a batch of 1\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PyTorch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# generate the output in inference mode (no gradient tracking)\n",
    "with torch.inference_mode():\n",
    "    batched_output_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "# extract the generated tokens by skipping the input ids\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] \n",
    "    for input_ids, output_ids \n",
    "    in zip(model_inputs.input_ids, batched_output_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unload Model from VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mem_stats():\n",
    "    import torch\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\n",
    "\n",
    "def reclaim_memory():\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "    for _ in range(3):\n",
    "        for gen in range(3):\n",
    "            gc.collect(gen)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "\n",
    "reclaim_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Gated Models from Hugging Face\n",
    "\n",
    "Some models on Hugging Face are gated and require a token to be downloaded.  \n",
    "\n",
    "Before downloading the model, You need to agree to the model's author terms of use.\n",
    "\n",
    "<div>\n",
    "<img src=\"./.images/hf_accept_model_tos.png\" alt=\"Hugging Face Gated Model\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Now, create a new readonly token at [Hugging Face](https://huggingface.co/settings/tokens/new?tokenType=read) and store in a secure place.  \n",
    "Then, set it in the `HF_TOKEN` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Please fill in your Hugging Face token below\")\n",
    "os.environ[\"HF_TOKEN\"] = input(\"Hugging Face token:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the gated model\n",
    "\n",
    "We will be loading `meta-llama/Llama-3.1-8B-Instruct`, so make sure you nagivate to https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct before proceeding and accept the terms of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# using device_map=\"auto\" will automatically shard the model on the GPU + CPU memory thanks to accelerate\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype='auto', device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype='auto', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your name?\"}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    batched_output_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] \n",
    "    for input_ids, output_ids \n",
    "    in zip(model_inputs.input_ids, batched_output_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming the output to stdout\n",
    "\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.generate(**model_inputs, max_new_tokens=512, streamer=streamer)\n",
    "\n",
    "del streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer)\n",
    "thread = Thread(\n",
    "    target=model.generate, \n",
    "    kwargs=model_inputs | {\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    thread.start()\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in streamer:\n",
    "    generated_text += new_text\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "thread.join()\n",
    "\n",
    "del streamer\n",
    "del thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mem_stats()\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "reclaim_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "Quantization is a technique to reduce the model size and improve the inference speed.\n",
    "\n",
    "Generating with the previous model might have been slow because it might have not fit in the GPU VRAM, having to be sharded across GPU, CPU, and possibly disk.\n",
    "\n",
    "To easily calculate the memory footprint of the model, take the model data type and multiply it by the number of parameters.  \n",
    "For example, `Llama-3.1-8B-Instruct` has a data type of `float16` (which is 2 bytes per parameter) and 8B parameters, so the memory footprint is 2 bytes/param * 8B params ~= 16GB.\n",
    "\n",
    "Let's use `bitsandbytes` to quantize the model on the fly while loading it.\n",
    "\n",
    "See the [Quantization documentation](https://huggingface.co/docs/transformers/main/en/quantization/overview) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "quantization_config_4bit = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config_8bit, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"cuda:0\")\n",
    "\n",
    "print_mem_stats()\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "reclaim_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config_4bit, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"cuda:0\")\n",
    "\n",
    "print_mem_stats()\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "reclaim_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the memory footprint is reduced when using quantization, but its not a perfect liniar scale, as the model has to retain some unquantized parameters for operations reasons.\n",
    "\n",
    "`INT8` quantization:  \n",
    "**Expected:** `1byte/param * 8B params = 8GB`  \n",
    "**Actual:** `~9GB`  \n",
    "\n",
    "`INT4` quantization:  \n",
    "**Expected:** `0.5byte/param * 8B params = 4GB`  \n",
    "**Actual:** `~6GB`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization is a trade-off between model size, model quality and inference speed.  \n",
    "\n",
    "If the model fits in VRAM, it will be faster to run inference without quantization.  \n",
    "Quantization will allow you to run inference with a smaller model size, but can be slower, depending on the hardware, model architecture, and model sharding on multiple devices.  \n",
    "\n",
    "Quantizing to 8-bits usually does not come at a significant quality loss for the model.  \n",
    "Quantizing to 4-bits can come at a larger quality loss, but can still be useful if you need to run the model on a low resource device.\n",
    "\n",
    "Let's see how much faster we can run inference with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def catchtime(message: str = \"Time\"):\n",
    "    t1 = t2 = perf_counter() \n",
    "    yield lambda: t2 - t1\n",
    "    t2 = perf_counter() \n",
    "    print(f'{message}: {t2 - t1:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype='auto', device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype='auto',device_map=\"auto\")\n",
    "\n",
    "with catchtime('Time without quantization'):\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(5):\n",
    "            model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "reclaim_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config_8bit, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"cuda:0\")\n",
    "\n",
    "with catchtime('Time with 8-bit quantization'):\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(5):\n",
    "            model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "reclaim_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config_4bit, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"cuda:0\")\n",
    "\n",
    "with catchtime('Time with 4-bit quantization'):\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(5):\n",
    "            model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "reclaim_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using multimodal models\n",
    "\n",
    "We will be using `Qwen/Qwen2-VL-7B-Instruct` for this example.  \n",
    "This model is a multimodal model that can understand images as well as text.  \n",
    "\n",
    "The model is 8.3B parameters, requiring ~16.6GB of VRAM to load.  \n",
    "Since we are constrained by VRAM, we will be using a pre-quantized version of the model from `Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the optimum and auto-gptq libraries\n",
    "# quantized qwen model depends on them\n",
    "!pip install -q optimum auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype='auto',\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "display(image.resize((int(image.width / image.height * 300), 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = image.resize((int(image.width / image.height * 300), 300))\n",
    "display(resized_image)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs = inputs = processor(\n",
    "    text=[input_text], images=[resized_image], padding=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=128)\n",
    "print(processor.decode(output[0][len(inputs['input_ids'][0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del processor.tokenizer\n",
    "del processor\n",
    "reclaim_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured outputs\n",
    "\n",
    "When generating the next token, models have a vector of probabilities representing the likelihood of each token in the vocabulary to be the next token.  \n",
    "We will use a library that constrains the generation process to produce valid JSON output.\n",
    "\n",
    "We will be using the [`outlines`](https://github.com/dottxt-ai/outlines/) library to constrain the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's currently a bug in the outlines library \n",
    "# during the installation process it assumes that rust is already installed\n",
    "# in colab and other linux environments this is not the case\n",
    "# so we need to install rust first\n",
    "import os\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "!source $HOME/.cargo/env\n",
    "os.environ['PATH'] += f':{os.environ[\"HOME\"]}/.cargo/bin'\n",
    "!echo $PATH\n",
    "!pip install \"outlines==0.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# load the model\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=\"auto\", device_map=\"cuda:0\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from IPython.display import display\n",
    "\n",
    "image_url = \"https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "display(image.resize((int(image.width / image.height * 400), 400)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines.processors.structured import JSONLogitsProcessor\n",
    "from outlines.models.transformers import TransformerTokenizer\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "# create a Pydantic schema for the output\n",
    "class BackpackFeatures(BaseModel):\n",
    "    feature: str = Field(description=\"A feature of the backpack\")\n",
    "\n",
    "class ImageDetails(BaseModel):\n",
    "    title: str = Field(description=\"The title of the image\")\n",
    "    features: list[BackpackFeatures] = Field(description=\"A list of features of the backpack\")\n",
    "\n",
    "# convert the schema to a JSON schema\n",
    "schema = ImageDetails.model_json_schema()\n",
    "schema_text = json.dumps(schema)\n",
    "\n",
    "# create a JSON logits processor\n",
    "json_logits_processor = JSONLogitsProcessor(schema, TransformerTokenizer(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = image.resize((int(image.width / image.height * 400), 400))\n",
    "display(resized_image)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": f\"\"\"\n",
    "Describe the image in detail. \n",
    "\n",
    "Respond in valid JSON, formatted following the schema: {schema_text}\n",
    "\"\"\".strip()}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs = inputs = processor(\n",
    "    text=[input_text], images=[resized_image], padding=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=512, logits_processor=[json_logits_processor])\n",
    "# output = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "json_text = processor.decode(output[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "json_data = json.loads(json_text)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
